{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 Neural Network and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5.1 NN Model on MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initial Setup\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "L1 = 200\n",
    "L2 = 100\n",
    "L3 = 60\n",
    "L4 = 30\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L1], stddev=0.1))\n",
    "B1 = tf.Variable(tf.zeros([L1]))\n",
    "W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([L2]))\n",
    "W3 = tf.Variable(tf.truncated_normal([L2, L3], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([L3]))\n",
    "W4 = tf.Variable(tf.truncated_normal([L3, L4], stddev=0.1))\n",
    "B4 = tf.Variable(tf.zeros([L4]))\n",
    "W5 = tf.Variable(tf.truncated_normal([L4, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Step 2: Setup Model\n",
    "Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2, W3) + B3)\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "yhat = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# Step 3: Loss Functions\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y)\n",
    "\n",
    "# Step 4: Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "# optimizer = tf.train.AdamOptimizer(0.1)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "is_correct = tf.equal(tf.argmax(y,1),tf.argmax(yhat,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i in range(int(mnist.train.num_examples /100)):\n",
    "        batch_X, batch_y = mnist.train.next_batch(100)\n",
    "        train_data = {X: batch_X, y: batch_y}\n",
    "        sess.run(train, feed_dict=train_data)\n",
    "        print(\"Training Accuracy = \", sess.run(accuracy, feed_dict=train_data))\n",
    "\n",
    "# Step 5: Evaluation\n",
    "test_data = {X:mnist.test.images,y:mnist.test.labels}\n",
    "print(\"Testing Accuracy = \", sess.run(accuracy, feed_dict = test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Convert the label into one-hot vector\n",
    "num_labels = len(np.unique(target))\n",
    "Y = np.eye(num_labels)[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initial Setup\n",
    "X = tf.placeholder(tf.float32, [None, 4])\n",
    "W = tf.Variable(tf.zeros([4, 3]))\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# Step 2: Define Model\n",
    "yhat = tf.matmul(X, W) + b\n",
    "y = tf.placeholder(tf.float32, [None, 3]) # Placeholder for correct answer\n",
    "\n",
    "# Step 3: Loss Function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=yhat))\n",
    "\n",
    "# Step 4: Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(yhat, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Step 5: Training Loop\n",
    "for epoch in range(150):\n",
    "    for i in range(len(train_X)):\n",
    "        training_data = {X: train_X[i: i + 1], y: train_Y[i: i + 1]}\n",
    "        sess.run(train, feed_dict = training_data)\n",
    "\n",
    "# Step 6: Evaluation\n",
    "testing_data = {X: test_X, y: test_Y}\n",
    "print(\"Training Accuracy = \", sess.run(accuracy, feed_dict = testing_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
